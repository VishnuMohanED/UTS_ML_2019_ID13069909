{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishnuMohanED/UTS_ML_2019_ID13069909/blob/master/Assignment1_ID13069909.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2naTmCKR47Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XurYMFMSh2V",
        "colab_type": "text"
      },
      "source": [
        " \n",
        "32513 - Advanced Data Analytics Algorithms\n",
        "“Generative Adversarial Networks “\n",
        "\n",
        "Review Report\n",
        "\n",
        "Subject Coordinator: Dr. Jun Li\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Submitted By\n",
        "Vishnu Mohan Edala Dhanraj    \t13069909\n",
        "\n",
        "\n",
        "INTRODUCTION\n",
        "Deep learning mainly emphasizes on developing rich and hierarchical models that provides probability distributions over natural images, symbols and waveforms in natural language processing systems. The term Adversarial refers to two nets pitting one against the other. The generative models earlier used had several difficulties such as difficulty in providing approximate probabilistic computations and find likelihood strategies. But this adversarial net framework developed by Ian J. Goodfellow provides a novel attempt of generative model G pitted against an adversary i.e. a discriminative model that learns or predicts where actually the sample arises from the data or the model distributions. This Can be kept analogous to the counterfeiters who are trying to submit fake currency without being detected and police trying to detect the same. This improves both the teams to implicate their methods and be better than one another. This article mainly focuses on training algorithms for different kinds of models by using back propagation and dropout algorithms. Through these methods it is evident that the author had provided detailed explanations on all those algorithms and contributed a sophisticated solution to problems that actually requires a generative conclusion such as image-image translation. The GAN’s potential is way huge as they can mimic any distribution of data ranging from different means music, images, speech and prose. \n",
        "Yann LeCun has stated GANs as “The Coolest Idea in deep learning in the last 20 years”.\n",
        "The aim of this paper is to review about GANs and analyze how it’s being used to find data distribution over natural language corpora.\n",
        "\n",
        "CONTENT\n",
        "BACKGROUND\n",
        "\n",
        "Firstly, before getting into General Adversarial Nets, relative works are discussed. This will give the viewer about all the insights about the purpose and its actual usage. An alternative to those directed graphical models along with latent variables are restricted Boltzmann machines RBMs, deep Boltzmann machines and their other variants.  The regular interactions between these unnormalized functions are rather normalized by global integration of random variables at any state.\n",
        "Deep belief networks (DBNs) are hybrid models that generally possess a single undirected layer and several directed layers. When fast approximate training criterion exists, DBNs experience computational difficulties with both directed as well as undirected models. Alternative models such as score matching as well as noise-contrastive estimation (NCE) are also been discussed. It is said that both these methods require probability density to be specified up to a normalization constant. Thus, all these generally define probability distribution explicitly at certain levels.\n",
        "Finally, some approaches are designed in such a way that these are trained by back propagation. These are termed as generative stochastic network (GSN) framework that possess generalized denoising auto-encoders. This attribute is seen in parameterized Markov chain as one learns the parameters of the machine that generate one iteration of Markov chain. Adversarial nets don’t require Markov chain for sampling as they don’t require feedback loops during generation. But they possess issues while generating in the feedback loop.\n",
        "\n",
        "TITLE : GENERATIVE ADVERSARIAL NETS\n",
        "\n",
        "Generative Adversarial Nets (GANs) is a new framework designed to generate modeling using deep learning methods i.e. Convoluted neural networks. This framework is an unsupervised learning method in machine learning in which it involves discovering and learning the patterns in the given set of data automatically and can be used to generate new samples that could have been ultimately drawn from the original dataset. This framework follows an adversarial process that eventually trains two models simultaneously. A generative model G that we train to capture data distributions, generate new samples and a discriminative model that that estimates that the sample comes mostly from the training data than the generative model G. In case of any arbitrary functions G and D there exists a unique solution, where G recovers the training data distribution and D equals ½.  If they are defined by multilayer perceptron’s the entire system can be trained with back propagation.\n",
        "This project mainly discusses about the novel framework, “Generative Adversarial Nets” by authors Ian J. Goodfellow, Jean Pouget-Abadie∗, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair†, Aaron Courville, Yoshua Bengio‡ published in Neural Information Processing Systems Conference (2014). The article will further be reviewed based on its application, technical quality, experiments as well as results.\n",
        "INNOVATION\n",
        "The Adversarial model is straight forward model when both the models are multilayer perceptron’s. \n",
        "Generative Models can be applied for,\n",
        "•\tgenerating realistic artwork samples such as Image, video, audio, etc.\n",
        "•\tDatasets that uses statistical inference\n",
        "•\tPlanning and simulation using Time-series data.\n",
        "The author’s general representation of GANs can be shown by the following figure.\n",
        " \n",
        "It mostly comprises of two models, Generator- that generates data samples similar to that of expected one and Discriminator- that recognizes whether the given input is real/fake.\n",
        "The Value function of Minimax Game played by generator and discriminator can be explained in the forthcoming sections. The parameters are listed as follows, \n",
        "Data - x\n",
        "Input noise variables - pz(Z)\n",
        "Data space mapping - G (z: θg) where G is differential function with θg as its parameters\n",
        "Second Layer perceptron - D (x: θd)\n",
        "D(x) - probability of x from data.\n",
        "When both D and G are trained in such a way that, D is trained to maximize the probability of assigning samples from G and simultaneously G is trained to minimize the log(1-D(G(z)).\n",
        "Thus the minmax game Of data and generative models D & G with value function V (G, D) can be given as,\n",
        "\n",
        " \n",
        "\n",
        "THEORETICAL ANALYSIS\n",
        "The adversarial nets are further theoretically explained by the author with these graph representations as shown in the forthcoming sections. The Discriminative distribution D is represented in the Blue, whereas the data generating distribution Px is represented in black dotted line to be differentiated from the generative distribution Pg which is by green and a solid line. \n",
        "The upward arrows symbolically represent the mapping x = G(z) which imposes the non-uniform distribution over the samples.\n",
        " \n",
        "Figure a - Considering an adversarial pair i.e. Pg is similar to Pdata. Both discriminative distribution D as well as generative distribution are partially accurate classifier.\n",
        "Figure b - Considering the Inner loop, the Distribution D is trained to discriminate the sample from data.\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "Figure C - the Gradient of the distribution has guided the G(z) to flow through the regions that are likely to be classified as the data.\n",
        "Figure D- After a series of iterations in training, both G and D reaches a point where they have enough capacity and cannot improve as Pg = Pdata. The discriminator has failed to differentiate the different between discriminative as well as generating distributions.\n",
        "\n",
        "TECHNICAL QUALITY\n",
        "The overall quality of this paper is of high standards. The author discussed the methodologies implied in this work and explained all the algorithms and the logic factors behind it. The author has explained each segment like generator, discriminator and noise generated during the modelling and also evaluated and calibrated its functions whenever required.\n",
        "The author had explained the algorithms that this minimax games possess a global optimum for Pg = P data and optimization of the same algorithm. Thus, achieving the desired result. The author did graphical representations with different lines and textures to differentiate the samples from the generating data distribution. Moreover, the non-uniform distribution Pg on those transformed samples are being represented by the upward arrows for better visibility.\n",
        "This paper was published in 2014 and this paper has been the base for several applications. Some of the major applications include generating photographs of human faces, Image-Image Translation, Text-image translation, face frontal view generations, photo editing, photo blending, video prediction, 3D object generation.\n",
        "The author trained the adversarial nets on a range of datasets such as MNIST, Toronto face database and CIFA and provided better results. Though he doesn’t claim that these trained samples are way better than the sample produced by the existing methods, he believes that these at least stay competitive with those better models. Thus, he concludes that GAN has much more potential than the other models.\n",
        "A clear comparison of major operations involved in each of the approaches in deep generative modeling is explained. Various factors such as training, inference, sampling and evaluating methods are considered. This clearly explains the major advantage that the adversarial models have compared with other existing models. The developed model or framework is best within the efforts of the author and it largely contributes to the other factors in the successive years.\n",
        "X-FACTOR\n",
        "In These GANs, both the generator as well as discriminator models are being modelled with neural networks, no difficulties in sampling, a gradient optimization algorithm will be used to train further.\n",
        "\n",
        "STEPS TO TRAIN GAN\n",
        "The author has explained certain steps to train a GAN and are as follows, \n",
        "•\tSample a real data set a noise set as well, each with similar size m\n",
        "•\tTrain the discriminator onto this data\n",
        "•\tSample another noise subset, in linear size with the noise as well as real data set.\n",
        "•\tTrain the generator onto this data.\n",
        "•\tRepeat the procedures until both have enough capacity and cannot improve further.\n",
        "\n",
        "TIPS TO TRAIN  GAN\n",
        "The author here explains about certain tips in training a GAN. He iterates that while training the discriminator, the generator values are held to be constant and similarly while training the generator the discriminator values is said to be constant. Each module should be trained in against with a static adversary. Both of them try to empower the other. If the discriminator model is too good, it will empower with values so either 0 or 1 so that the generator will struggle in reading that gradient again with it. In the same way, if the generator is good, it will lead the discriminator to false negatives by persistently exploiting the weakness.\n",
        "The two networks should have similar skill level such that they have similar learning rates.\n",
        "GANs can be represented as shown, the generator considers random simple input variables and produce new data. The discriminator considers generated true data and discriminate them by building a classifier. The major functionality of the generator is to fool the discriminator and the discriminator is to find the difference between true and generated data.\n",
        " \n",
        "\n",
        "Firstly, we will be using stochastic gradient, as this algorithm has been proven successful in various fields.\n",
        "For n - number of training iterations and for K steps do, \n",
        "Considering Sample minibatch of noise samples from Pg(z) and data generating distribution Pdata(x) \n",
        " Then updating the discriminator by its stochastic gradient can be given by,\n",
        " \n",
        "In the same way, updating the generator samples by descending its stochastic gradient.\n",
        "\n",
        " \n",
        "Further he explains about the growth optimality of D for any generator G. Moreover, one important criterion which differs from GAN from other approaches is that G need not be invertible. Thus the training of D for any G value is mainly to maximize the quantity of v(G, D) as shown,\n",
        " \n",
        "\n",
        "Additionally, the author explains about the convergence of algorithm 1, such that when both G and D has enough capacity and there is no room for any further change, Pg is updated to improve the criterion which eventually make pg converge to the Pdata.\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "APPLICATIONS OF GAN\n",
        "GAN have been used in most of the real-life applications like text/video/image generation, text to image conversions and drug discovery.The author Ian Goodfellow has implied this generative adversarial network on two datasets. MNIST and TFD (Toronto face dataset). For both these datasets, the column on their right possess the right or true data that are almost or near to that of the generated samples or its neighbors. This emphasizes that the data produced are really generated based on the generator and discriminator methods and or not memories by any means on the network.\n",
        " \n",
        "\n",
        "Considering the Dataset MNIST, which consists of handwritten 60000 black and white images that are of 28x28 pixel.\n",
        "\n",
        " \n",
        "\n",
        "Upon training, the images tend to be like shown below, as they are of pure noise,\n",
        " \n",
        "Later the images try to improve gradually and can be visualized as shown below,\n",
        " \n",
        "Finally, good images start to show as shown below,\n",
        " \n",
        "\n",
        "While there is no claim made that the samples generated by GAN are better than the existing ones, there should be a statement made that the models or samples developed are at least competitive with the better generative models and also the potentiality of the adversarial framework needs to be addressed.\n",
        "Further by using GAN, lot of applications has been discovered post 2014 and some of the interesting ones are listed.\n",
        "•\tZhifei Zhang has proposed a paper “Age Progression/Regression by Conditional Adversarial Autoencoder” where he has utilized GAN method to de-age the faces in photographs.\n",
        "•\tHe Zhang, et al in 2017 has proposed a methodology to remove snow and rain from the photographs by using GAN for photo editing.\n",
        "•\tYaniv Taigman, et al in 2016 has utilised GAN to translate images from one domain to another. For Eg. Photos to emojis or cartoon faces.\n",
        "PRESENTATION QUALITY\n",
        "Ian J. Goodfellow has presented this article with high standards and coherence. The problem statement was cleared considered, analyzed and addressed. The article has been approached well enough and each section have been fairly explained with detailed explanations about the methodologies or techniques involved.\n",
        "Apparently, when the sequence of the paper is considered it was good and easy to follow till the end. This is mainly because the article has fairly explained all the cases involved and explained them with both theoretical as well as experimental results.\n",
        "Moreover, the author has clearly stated what the existing methodologies or models possess, what are their limitations briefly and explained why GANs can replace them with better results. This shows the groundwork and detailing that he had while making this article. The experimental results with sample datasets MNIST, TFD and CIFAR 10 are all explained and well presented. The algorithm explanations were clear and concise. Moreover, the article had deep groundwork with good number of references that explains all the techniques involved in this work.\n",
        "OBSERVATIONS FROM THIS ARTICLE\n",
        "This framework and its usage come with both advantages as well as disadvantages based on in pre-existing frame models and its working. \n",
        "This framework has some statistical advantage that the generator network doesn’t need to be updated directly with the data examples but only with the discriminator. This explains that the components are not directly copied with the generator’s parameters.\n",
        "Another greatest advantage of adversarial nets is that it produces sharp and even distributions whereas, other methods like Markov chains produces blurred distributions.\n",
        "Some of the PROS and CONS are discussed briefly as shown below,\n",
        "•\tGANs are most suitable for training classifiers in a semi-supervised way.\n",
        "•\tGANs generate samples much faster than the other visible belief nets like pixelRNN, WaveNet as there is no need to sequential sample generation.\n",
        "•\tGANs doesn’t need MCMC approximations to train the samples. Most of the people claim that GANs are unstable and are hard to train. But instead they are much easier to train than other machines which struggles when it comes to high dimensional spaces.\n",
        "•\tGANs generally require only one time to pass through the model, rather than other machines which utilizes number of iterations.\n",
        "\n",
        "LIMITATIONS OF GAN\n",
        "The primary disadvantage is that Pg(x) is not represented explicitly. The generative distribution G must be synchronized well enough with the data D during modelling.\n",
        "•\tFirstly, GANs are hard to train discrete data such as text.\n",
        "•\tTraining a GAN usually need to find the Nash equilibrium of the game. The gradient descent does this work at times and fails at some cases. This factor makes the GAN unstable when compared with other training methods.\n",
        "•\tWhen there Is no balance between generator and discriminator , it ends in overfitting and finally these are highly sensitive to hyperparameters.\n",
        "\n",
        "CONCLUSCION\n",
        "As per all the experiments made and applications discussed from above, GANs  overcome certain limitations that other learners experience .The author admits that this framework possess too many straightforward extensions such as efficiency improvements in training the sample Z by determining better distributions or by better methods of corelating G and D, semi-supervised learning  and certain factors that concludes that this paper possess great directions and viability for the further researching methods.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "REFERENCES\n",
        "John Romero, A Beginner's Guide to Generative Adversarial Networks (GANs), viewed 27 Aug 2019, <https://skymind.ai/wiki/generative-adversarial-network-gan>\n",
        "\n",
        "Diego Gomez Mosquera 2018, GANs from Scratch 1: A deep introduction. With code in PyTorch and TensorFlow, Medium, viewed 26 Aug 2019, <https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f>\n",
        "\n",
        "Joseph Rocca 2019, Understanding Generative Adversarial Networks (GANs), Medium, viewed 27 Aug 2019, < https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29>\n",
        "\n",
        "Jason Brownlee 2019, A Gentle Introduction to Generative Adversarial Networks (GANs), Machine Learning Mystery, viewed 28 Aug 2019, <https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/>\n",
        "\n",
        "Ian Goodfellow 2016, Generative Adversarial Networks (GANs), OpenAI, Barcelona, viewed 27 Aug 2019,<https://media.nips.cc/Conferences/2016/Slides/6202-Slides.pdf>\n",
        "\n",
        "Zhifei Z., Yang S. , Hairong Q 2017, Age Progression/Regression by Conditional Adversarial Autoencoder, The University of Tennessee, Knoxville, TN, USA, viewed 28 Aug 2019, <https://arxiv.org/pdf/1702.08423.pdf> \n",
        "\n",
        "He Zhang, Vishwanath Sindagi, Vishal M. Patel 2019, Image De-raining Using a Conditional Generative Adversarial Network, IEEE, viewed 28 Aug 2019, <https://arxiv.org/pdf/1701.05957.pdf>\n",
        "\n",
        "Yaniv T., Adam P. & Lior W. 2016, UNSUPERVISED CROSS-DOMAIN IMAGE GENERATION, ICLR, Tel-Aviv, Israel, viewed 28 Aug 2019, < https://arxiv.org/pdf/1611.02200.pdf>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn3FjOR9Svoq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}